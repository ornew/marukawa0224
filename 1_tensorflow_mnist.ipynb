{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e45f254d-0250-4b0c-a034-bfd211f19762"
    }
   },
   "source": [
    "# MNIST by TensorFlow on SageMaker\n",
    "\n",
    "<small>Copyright © 2018 by Arata Furukawa. (http://ornew.net)</small>\n",
    "\n",
    "<small style=\"font-size:0.5em;\">この資料は「機械学習アプリを「賢く」作る：Amazon SageMakerクラウド・ハンズオン」のために作成されたものです。ノートブックをダウンロードし、持ち帰って自由に実行・加工していただいて構いませんが、解説文の外部への公開・転載はご遠慮ください。</small>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは、「ディープラーニングの\"Hello world\"」と呼ばれるMNISTの実装を通して、以下の2つを学びます。ついでにJupyterの使い方も解説します。\n",
    "\n",
    "- SageMakerでの独自モデル開発プロセス\n",
    "- TensorFlowの使い方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d011f340-07f5-431d-82f0-397a7221949a"
    }
   },
   "source": [
    "Jupyter上でグラフを表示するためのおまじないです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 's3://marukawa0224-XX/tensorflow'\n",
    "base_job_name = 'marukawa0224-XX-mnist-job'\n",
    "print('output_path:\\n\\t{}'.format(output_path))\n",
    "print('base_job_name:\\n\\t{}'.format(base_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "9e59d6ae-314d-4a66-bb79-4c7b9f7a16ae"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4d169f0b-00ff-4839-9db1-39a1a34fc7b8"
    }
   },
   "source": [
    "TensorFlowをインポートします。慣習的に、tfという名前でインポートすることが多いです。\n",
    "\n",
    "SageMakerでは現在1.4.0がサポートされています。バージョンが1.4.0である事を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4b2f7e3b-f970-4979-a16c-c0938ba7d2f1"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMakerでTensorFlowモデルが動く流れ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowの高レベルAPIの**Estimator**という仕様が（ほぼ）そのままSageMakerで実行できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowのEstimatorは、\n",
    "\n",
    "- 学習（training）\n",
    "- 評価（evaluation）\n",
    "- 予測（prediction）\n",
    "- サービング（APIサーバ化）のためのモデルのエクスポート\n",
    "\n",
    "を担う、機械学習モデルの高水準なインターフェイスです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予めTensorFlowに用意されたEstimatorを使う他、**EstimatorSpec**の要件を満たすことで、自由なモデルを使った**オリジナルのEstimatorを作る**ことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EstimatorをSageMakerのAPIに渡すことで、\n",
    "\n",
    "- 学習や評価のためのインスタンスを実行\n",
    "- 予測を行うAPIエンドポイント作成、公開\n",
    "\n",
    "などが容易に行なえます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 名前 | 役割\n",
    ":----|:----\n",
    " tensorflow.estimator.Estimator | モデルの学習、評価、予測、出力などをするインターフェイス\n",
    " tensorflow.estimator.EstimatorSpec | Estimatorとして機能するために必要なもの\n",
    " sagemaker.tensorflow.TensorFlow | Estimatorの実行に必要なものを定義したプログラムを渡すと、実際のクラウド環境に自動で展開して実行する事ができる\n",
    "\n",
    "カスタムされたEstimatorの作成の詳細は、[公式ガイド](https://www.tensorflow.org/get_started/custom_estimators)も参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMakerに渡すプログラムの要件\n",
    "\n",
    "SageMakerに渡すプログラムには、以下の4つの関数が定義されている必要があります。\n",
    "\n",
    "1. `train_input_fn`\n",
    "2. `eval_input_fn`\n",
    "3. `serving_input_fn`\n",
    "4. `model_fn` または `keras_model_fn` または `estimator_fn` のいずれか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 名前 | 役割\n",
    ":----|:----\n",
    " train_input_fn | 学習データの読み込みと前処理をする関数です。\n",
    " eval_input_fn | 評価データの読み込みと前処理をする関数です。\n",
    " serving_input_fn | 学習やサービング時にモデルの入力となるプレースホルダなどを定義します。\n",
    " model_fn | EstimatorSpecを返す関数です。独自モデルを使いたいときはこれを定義します。\n",
    " keras_model_fn | Kerasで実装する場合はこの関数でKerasのEstimatorを返します。\n",
    " estimator_fn | 予めTensorFlowで定義されている簡単なEstimatorを使うときは、この関数でEstimatorを返します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "別の言い方をすれば、**たった4つ関数を定義するだけ**でSageMakerで動かすモデルは完成です！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にこれらの関数を簡単に実装していきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・評価の入力データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**サンプル**の要件を定めます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 名前 | 定義\n",
    ":----|:----\n",
    " フィーチャ | モデルの入力データのこと。\n",
    " サンプル  | 学習・評価データのこと。\n",
    " データセット | サンプルの集合のこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_SPEC = {\n",
    "    'image': tf.FixedLenFeature([28 * 28], tf.float32),\n",
    "    'label': tf.FixedLenFeature([]       , tf.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルデータの構築・アップロード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習・評価までのサンプルデータの流れは以下のようになります。\n",
    "\n",
    "1. データセットを用意する\n",
    "2. 各データをtf.Exampleにシリアライズする\n",
    "3. シリアライズしたデータセットをTFRecordとして保存する\n",
    "4. **事前にS3バケット上にアップロードする**\n",
    "5. 学習等を実行するときにS3パスを指定する\n",
    "6. クラウド上の実行コンテナにS3パスのデータがマウントされる\n",
    "7. `*_input_fn`関数の第一引数にマウントパスが渡される\n",
    "8. TFRecordファイルを読み込む\n",
    "9. デシリアライズされフィーチャとしてモデルに入力される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowでは、データセットのファイル形式として**TFRecord**を推奨しています。TFRecordは大規模データを省メモリ＆並列に処理することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを準備し`EXAMPLE_SPEC`に合わせて変換しなくてはいけません。この作業はモデルの開発では最初に1回行うもので、その後も頻繁に行うものではありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eaad1b16-141c-41e4-b9a2-34ac9b9090e4"
    }
   },
   "source": [
    "TensorFlowにはチュートリアル用にMNISTデータセットをダウンロードするメソッドが用意されているので、まずはデータセットをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Example`に変換し、シリアライズして`TFRecordWriter`で書き込みます。長く見えますが、やっていることは単純です。`feature`の形式が`EXAMPLE_SPEC`と一致していることが重要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tfrecord(filename, images, labels):\n",
    "    options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    with tf.python_io.TFRecordWriter(filename, options) as tfrecord:\n",
    "        for image, label in zip(images, labels):\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'image' : tf.train.Feature(\n",
    "                    float_list=tf.train.FloatList(value=image.flatten() / 255.)),\n",
    "                'label' : tf.train.Feature(\n",
    "                    int64_list=tf.train.Int64List(value=[np.argmax(label)])),\n",
    "            }))\n",
    "            tfrecord.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.gfile.MakeDirs('data/mnist')\n",
    "convert_tfrecord('data/mnist/train.tfr', train[0], train[1])\n",
    "convert_tfrecord('data/mnist/test.tfr' , test[0] , test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data/mnist`ディレクトリに`train.tfr`と`test.tfr`のファイルができていれば成功です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルデータの読み込みとフィーチャへの変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先程「SageMakerに渡すプログラムの要件」で確認した関数のうち、学習と評価のためのデータ入力部分を書きましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3上にアップロードされたTFRecord形式のデータを読み込みます。TensorFlow Dataset APIを使うと簡単に扱うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_input_fn(data_dir, params):\n",
    "    tfrecord = os.path.join(data_dir, 'test.tfr')\n",
    "    return _input_fn(tfrecord, params, training=True)\n",
    "    \n",
    "def eval_input_fn(data_dir, params):\n",
    "    tfrecord = os.path.join(data_dir, 'test.tfr')\n",
    "    return _input_fn(tfrecord, params, training=False)\n",
    "\n",
    "def _parse_proto(proto):\n",
    "    parsed = tf.parse_single_example(proto, EXAMPLE_SPEC)\n",
    "    image = parsed['image']\n",
    "    label = tf.one_hot(parsed['label'], 10)\n",
    "    return image, label\n",
    "\n",
    "def _input_fn(tfrecord, params, training):\n",
    "    dataset = (tf.data.TFRecordDataset(tfrecord, compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "        .map(_parse_proto)\n",
    "        .shuffle(1000))\n",
    "    # 学習のときは無限に生成する\n",
    "    dataset = dataset.repeat() if training else dataset\n",
    "    images, labels = (dataset\n",
    "        .batch(params.get('batch_size', 512))\n",
    "        .make_one_shot_iterator()\n",
    "        .get_next())\n",
    "    return {'images': images}, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測時の入力データの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習、評価時のモデルの入力関数では実データを読み込みましたが、予測をAPI化してサービングの入力を受ける場合、実際にデータが来るまで中身がわかりません。プレースホルダを定義してフィーチャのデータ形式を明確にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn(hparams):\n",
    "    features = {\n",
    "        'images': tf.placeholder(tf.float32, [None, 784])\n",
    "    }\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(features)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimatorの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後にもう一つ、何らかのEstimatorを定義しなくてはいけません。試しに、ソフトマックス回帰モデルを実装してみましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):    \n",
    "    x = features['images']\n",
    "    \n",
    "    W = tf.get_variable('W', [784,10])\n",
    "    b = tf.get_variable('b', [10])\n",
    "    \n",
    "    y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "    y_indices = tf.argmax(input=y, axis=1) # 予測したラベルのインデクス\n",
    "    \n",
    "    predictions = {\n",
    "        'classes': y_indices,\n",
    "        'probabilities': y,\n",
    "    }\n",
    "    export_outputs = {\n",
    "        'predictions': tf.estimator.export.PredictOutput(predictions),\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            export_outputs=export_outputs)\n",
    "\n",
    "    learning_rate = params.get('learning_rate', 0.5)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        -tf.reduce_sum(labels * tf.log(y), reduction_indices=[1]))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    fit = optimizer.minimize(cross_entropy, global_step)\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=cross_entropy,\n",
    "        train_op=fit,\n",
    "        export_outputs=export_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "細かい実装は、学習を走らせているときに解説いたします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMakerで実行する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMakerを操作するために、セッションを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先程TFRecordを保存した`data`ディレクトリをS3にアップロードします。\n",
    "\n",
    "ただ、参加者全員がそれぞれにアップロードするとそれなりの容量になってしまうので、ハンズオンでは事前にアップロードしたものをご利用ください。もし持ち帰ってご自身で実行される場合は、以下のプログラムを実行すればアップロードできます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "data_dir = session.upload_data(\n",
    "    'data/mnist', # ローカルディレクトリ\n",
    "    '',           # バケット名を入れてください\n",
    "    'data/mnist') # S3パス\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前にこちらがアップロードしたS3パスです\n",
    "data_dir = 's3://sagemaker-seminar-data/mnist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Estimator`を作ります。この`Estimator`は、TensorFlowの`Estimator`をSageMakerのAPIでラップしたものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一引数に、`Estimator`の本体となるプログラムのパスを指定します。ここまでで定義した関数を事前に`code/mnist.py`ファイルに保存してありますので、そのパスを指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat code/mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'learning_rate': 0.5,\n",
    "    'batch_size': 512,\n",
    "}\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "estimator = TensorFlow(\n",
    "    entry_point='./code/mnist.py',\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    output_path=output_path,\n",
    "    code_location=output_path,\n",
    "    training_steps=1000,\n",
    "    evaluation_steps=100,\n",
    "    base_job_name=base_job_name,\n",
    "    \n",
    "    #########################################################\n",
    "    # ハンズオンでは以下の設定を絶対に変えないでください！！！\n",
    "    # 他の方が実行できなくなったり、高額な課金が発生することがあります。\n",
    "    # 異常な課金等を確認した場合、請求額を追加徴収させて頂きます。\n",
    "    #########################################################\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、学習を実行しましょう！\n",
    "\n",
    "estimatorのfitメソッドを呼び出すと、学習を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator.fit(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ソフトマックス回帰モデルの解説"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習には少し時間がかかるので、その間に`model_fn`とその中身の解説をします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_fn`の引数は4つです。\n",
    "\n",
    "### features\n",
    "\n",
    "入力となる特徴のデータです。今回は`'images'`キーで登録された784次元の数字を任意長のバッチ単位で受け取ります。\n",
    "\n",
    "### labels\n",
    "\n",
    "入力となるラベルのデータです。今回は、0から9までの数字のOHV表現です。\n",
    "\n",
    "OHV（One Hot Vector）とは、ベクトルの要素のうち1つだけが1であるようなベクトルです。OHV表現と数字の対応は以下のようになります。\n",
    "\n",
    "```\n",
    "                           OHV <==> Number\n",
    "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0] <==> 0\n",
    "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0] <==> 1\n",
    "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0] <==> 2\n",
    "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0] <==> 3\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0] <==> 4\n",
    "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0] <==> 5\n",
    "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0] <==> 6\n",
    "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0] <==> 7\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0] <==> 8\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1] <==> 9\n",
    "```\n",
    "\n",
    "１つのベクトルを、確率分布とみなすことができます。「`i`番目の値が大きいということは、数字`\"i\"`の確率が高い」と読み替えて頂いてもここでは差し支えありません。\n",
    "\n",
    "### mode\n",
    "\n",
    "実行モードです。`tf.estimator.ModeKeys.*`に定義される定数です。学習や評価で処理を分けることができます。\n",
    "\n",
    "### params\n",
    "\n",
    "実行するときに指定されるハイパーパラメータです。ここでは学習率だけを使っています。ハイパーパラメータはユーザが自由に設定することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "92e53af0-db6d-4b68-9ae6-f4a7627f06f6"
    }
   },
   "source": [
    "### 層の構築\n",
    "\n",
    "ニューラルネットワークは以下の式に準ずる層の積み重ねとみなされます。\n",
    "\n",
    "$$ y = \\phi (xW + b) $$\n",
    "\n",
    "- $x$\n",
    "    - 入力データ\n",
    "    - 今回は任意個の28x28=784ピクセルのグレースケール画像（1次元配列）\n",
    "- $W$\n",
    "    - 重みパラメータ\n",
    "    - 学習したい定数値\n",
    "    - 今回は入力との行列積\n",
    "- $b$\n",
    "    - バイアスパラメータ\n",
    "    - 学習したい定数値\n",
    "- $\\phi$\n",
    "    - 活性化関数\n",
    "    - 今回はsoftmax\n",
    "- $y$\n",
    "    - 出力\n",
    "    - 10個の値を持つ1次元配列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e3a49585-b808-411e-bd95-fd41caba04a9"
    }
   },
   "source": [
    "softmaxは、独立した固定長の分類器で頻繁に用いられます。\n",
    "\n",
    "$$ \\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $$\n",
    "\n",
    "これを活性化関数とすると、このモデルは以下の式で表せます。\n",
    "\n",
    "$$ y = \\text{softmax}(xW + b) $$\n",
    "\n",
    "実際に、TensorFlowで上記の式を実装した結果が、`model_fn`の下記になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "x = features['images']\n",
    "\n",
    "W = tf.get_variable('W', [784,10])\n",
    "b = tf.get_variable('b', [10])\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "518bad6f-3976-4ddd-884a-6d59ecc27191"
    }
   },
   "source": [
    "$W$と$b$は、学習したい**パラメータ（変数）**なので、**実行が終わっても値が保持されてほしい**ですね。こういう値は`tf.get_variable`を用いて作成します。\n",
    "\n",
    "フィーチャの形式は、`serving_input_fn`で`'tf.placeholder(tf.float32, [None, 784])`と定義しました。`[None,784]`は、値の形状を示します。**Noneは任意長を示すので、1次元目が任意長、2次元目が784個となる二次元配列**という意味です。\n",
    "\n",
    "`'W'`と`'b'`は変数名で、`[784,10]`と`[10]`は値の形状を示しています。\n",
    "\n",
    "\n",
    "$$\n",
    "y =  \\text{softmax}(\n",
    "\\begin{bmatrix}\n",
    "    \\begin{array}{cccc}\n",
    "      x_{1,1} & x_{1,2} & \\ldots & x_{1,784} \\\\\n",
    "      x_{2,1} & x_{2,2} & \\ldots & x_{2,784} \\\\\n",
    "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "      x_{n,1} & x_{n,2} & \\ldots & x_{n,784}\n",
    "    \\end{array}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\begin{array}{cccc}\n",
    "      W_{1,1} & W_{1,2} & \\ldots & W_{1,10} \\\\\n",
    "      W_{2,1} & W_{2,2} & \\ldots & W_{2,10} \\\\\n",
    "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "      W_{784,1} & W_{784,2} & \\ldots & W_{784,10}\n",
    "    \\end{array}\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    \\begin{array}{cccc}\n",
    "      b_{1} \\\\\n",
    "      b_{2} \\\\\n",
    "      \\vdots \\\\\n",
    "      b_{10}\n",
    "    \\end{array}\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$y$の式は、数式そのままですね。これでモデルが定義できました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの予測と、出力したい値を定義します。\n",
    "\n",
    "```python\n",
    "y_indices = tf.argmax(input=y, axis=1) # 予測したラベルのインデクス\n",
    "\n",
    "predictions = {\n",
    "    'classes': y_indices,\n",
    "    'probabilities': y,\n",
    "}\n",
    "export_outputs = {\n",
    "    'predictions': tf.estimator.export.PredictOutput(predictions),\n",
    "}\n",
    "```\n",
    "\n",
    "`argmax`は入力のうち**一番値が大きな要素のインデクス**を返します。今回、OHVがインデクスと対応しているので、そのまま**最も確率が高い数字**を示すことになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測モードのときはここで終了です。とても簡単でした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9e3b2517-b7b9-4e0b-a013-2e7723659195"
    }
   },
   "source": [
    "### 誤差を定義する\n",
    "\n",
    "モデルを学習したり、評価するときには、その方法を記述しなくてはいけません。\n",
    "\n",
    "モデルの出力$y$と、理想的な答えの出力が**どの程度違うのか**を定式化します。これを**誤差**（または**損失**）と呼びます。\n",
    "\n",
    "どの程度「違う」のかを示す**誤差が小さくなるほど、精度が高い**と言えますね。ディープラーニングでは、**誤差が小さくなるようにモデルの各パラメータを修正することで、モデルを学習させます**。\n",
    "\n",
    "誤差を定義しましょう。理想的な答え(ここではラベルと呼びます)を$y'$とします。2つの確率分布がどの程度違うのかを示すために、**交差エントロピー**という関数を使います。交差エントロピーは以下の式で定義されます。\n",
    "\n",
    "$$ H_{y’}(y) = -\\sum_i y’_i \\log(y_i) $$\n",
    "\n",
    "実装しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4a299e0b-12f0-40df-ad55-0fc307005cdf"
    }
   },
   "source": [
    "```python\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    -tf.reduce_sum(labels * tf.log(y), reduction_indices=[1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "606dafdd-81d4-4617-9b98-c691f71094ec"
    }
   },
   "source": [
    "任意の数のデータ入力を同時に受け取るため、それぞれのデータごとの誤差の平均を最終的な誤差とします。`reduce_mean`で平均を計算していることに注意してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7c2deb88-1460-4059-a5f1-babf67ede31b"
    }
   },
   "source": [
    "### 誤差を小さくするようにパラメータを最適化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "76a8cbe2-9108-48e6-9c57-e8e201c57298"
    }
   },
   "source": [
    "さて、誤差を定義したので、これを小さくするようにパラメータを修正しなくてはなりません。\n",
    "\n",
    "最適化アルゴリズムには様々な手法が存在しますが、ここでは全ての基本となる**勾配降下法**を用いましょう。\n",
    "\n",
    "基本的な最適化アルゴリズムは事前にTensorFlowに実装されています。勾配降下法によってパラメータを最適化するためには、`tf.train.GradientDescentOptimizer`を用います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "28f74350-939d-4175-a2ec-089a47666782"
    }
   },
   "source": [
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "fit = optimizer.minimize(cross_entropy, global_step)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d728c276-f3ce-43dc-872d-725d0e7c7929"
    }
   },
   "source": [
    "これで、学習率0.5の最急勾配降下法により、**交差エントロピーを最小化するように**全ての学習用パラメータを最適化する`fit`を定義しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EstimatorSpec`に、誤差や学習オペレーションなどを指定することで、学習と評価時の定義は終わりです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "return tf.estimator.EstimatorSpec(\n",
    "    mode=mode,\n",
    "    predictions=predictions,\n",
    "    loss=cross_entropy,\n",
    "    train_op=fit,\n",
    "    export_outputs=export_outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのデプロイ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の実行が正常に終わったら、モデルをデプロイしましょう。これも少し時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = estimator.deploy(\n",
    "    #########################################################\n",
    "    # ハンズオンでは以下の設定を絶対に変えないでください！！！\n",
    "    # 他の方が実行できなくなったり、高額な課金が発生することがあります。\n",
    "    # 異常な課金があった場合、請求額を追加徴収させて頂きます。\n",
    "    #########################################################\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.c2.xlarge')\n",
    "print(api.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測の実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デプロイしたモデルで実際に予測をしてみましょう！\n",
    "\n",
    "まずは予測機（Predictor）を作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.predictor import tf_serializer, tf_deserializer\n",
    "predictor = sagemaker.RealTimePredictor(endpoint=api.endpoint,\n",
    "                              deserializer=tf_deserializer, \n",
    "                              serializer=tf_serializer,\n",
    "                              content_type='application/octet-stream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict`メソッドを使うと、AWS上にデプロイされたTensorFlowモデルで予測を行うことができます。\n",
    "\n",
    "試しに、MNISTのテストデータセットの幾つかを推論してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_request(data):\n",
    "    from sagemaker.tensorflow.tensorflow_serving.apis import predict_pb2\n",
    "    tensor_proto = tf.make_tensor_proto(\n",
    "        values=data, shape=[1, 784], dtype=tf.float32)\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = 'generic_model'\n",
    "    request.model_spec.signature_name = 'predictions'\n",
    "    request.inputs['images'].CopyFrom(tensor_proto)\n",
    "    return request\n",
    "\n",
    "def predict(image, label):\n",
    "    result = predictor.predict(create_request(image))\n",
    "    predict = result.outputs['classes'].int64_val[0]\n",
    "    print('答え={} 予測={}'.format(label, predict))\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.randint(0, len(mnist.test.labels), 10):\n",
    "    image = mnist.test.images[i].tolist()\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    predict(image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に手書きをして予測をしてみましょう。\n",
    "\n",
    "※Chrome推奨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(np.asarray(data), '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowのモデルを実際にAPI化するとき、推奨されるのは**TensorFlow Serving**というツールを使うことです。実際、SageMakerでデプロイしたモデルはTensorFlow Servingサーバとして公開されます。\n",
    "\n",
    "TensorFlow Servingでは、**gRPC**という方式で通信を行います。通信データは**Protocol Buffers形式**です。先程のリクエストで作成しているのは、このProtocol Buffers形式のデータです。\n",
    "\n",
    "TensorFlow Servingが提供している`.proto`ファイルを用いることで、Android Javaや、C#、Node.jsなど、様々な言語からgRPC通信でTensorFlowモデルを利用することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したエンドポイントは、SDKを通して削除できます。次のノートブックに進む前に削除をお願いします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.delete_endpoint(api.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
